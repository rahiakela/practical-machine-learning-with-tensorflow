{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-4-regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/practical-machine-learning-with-tensorflow/blob/week-4/assignment_4_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JymdQZc0dl_T",
        "colab_type": "code",
        "outputId": "e2f816c6-6139-4dfd-a879-f66dad95cf40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "tf.random.set_seed(1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-rc0 in /usr/local/lib/python3.6/dist-packages (2.0.0rc0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0a20190806)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANupknPbdvdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston_dataset = load_boston()\n",
        "\n",
        "data_X = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
        "data_Y = pd.DataFrame(boston_dataset.target, columns=[\"target\"])\n",
        "data = pd.concat([data_X, data_Y], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_5-0VrueCPk",
        "colab_type": "code",
        "outputId": "682ff072-2ddc-40e6-9bac-5fe90a9cf77b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
        "train, val = train_test_split(train, test_size=0.2, random_state=1)\n",
        "print(len(train), \"train examples\")\n",
        "print(len(val), \"validation examples\")\n",
        "print(len(test), \"test examples\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "323 train examples\n",
            "81 validation examples\n",
            "102 test examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMdeRJpOhoDy",
        "colab_type": "code",
        "outputId": "1961b365-3b00-45e4-a3b5-333c1cbc6b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "# analyse dataset\n",
        "data_X.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n",
              "0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n",
              "1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n",
              "2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n",
              "3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n",
              "4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfZjyO27h8i8",
        "colab_type": "code",
        "outputId": "3d696563-ffd0-48f8-eaac-4eb24fc3fb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "source": [
        "data_Y.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   target\n",
              "0    24.0\n",
              "1    21.6\n",
              "2    34.7\n",
              "3    33.4\n",
              "4    36.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCLi-uf8eEzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the Pandas DataFrames into Tensorflow Datasets\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('target')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.batch(batch_size)\n",
        "  return ds\n",
        "\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, True, batch_size)\n",
        "val_ds = df_to_dataset(val, False, batch_size)\n",
        "test_ds = df_to_dataset(test, False, batch_size)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSWiWEYNtGeQ",
        "colab_type": "text"
      },
      "source": [
        "7) Define all the feature columns to be numerical columns, build and compile the model based on the instructions in the notebook and train\n",
        "\t   the model for 200 epochs. What is the range of the training error at the end of the training process?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXYhZDxbeh5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "510d6bff-d888-4c7e-c830-4a1a1991fb25"
      },
      "source": [
        "from tensorflow import feature_column\n",
        "# Defining Feature Columns\n",
        "CRIM = feature_column.numeric_column('CRIM')\n",
        "ZN = feature_column.numeric_column('ZN')\n",
        "INDUS = feature_column.numeric_column('INDUS')\n",
        "CHAS = feature_column.numeric_column('CHAS')\n",
        "NOX = feature_column.numeric_column('NOX')\n",
        "RM = feature_column.numeric_column('RM')\n",
        "AGE = feature_column.numeric_column('AGE')\n",
        "DIS = feature_column.numeric_column('DIS')\n",
        "RAD = feature_column.numeric_column('RAD')\n",
        "TAX = feature_column.numeric_column('TAX')\n",
        "PTRATIO = feature_column.numeric_column('PTRATIO')\n",
        "B = feature_column.numeric_column('B')\n",
        "LSTAT = feature_column.numeric_column('LSTAT')\n",
        "# define feature_columns as a list of features using functions from tf.feature_column\n",
        "feature_columns = []\n",
        "for header in ['CRIM', 'ZN', 'INDUS', 'CHAS',\t'NOX', 'RM', 'AGE', 'DIS', 'RAD',\t'TAX', 'PTRATIO',\t'B', 'LSTAT']:\n",
        "  feature_columns.append(feature_column.numeric_column(header))\n",
        "#for header in [CRIM,\tZN,\tINDUS,\tCHAS,\tNOX,\tRM,\tAGE,\tDIS,\tRAD,\tTAX,\tPTRATIO,\tB,\tLSTAT]:\n",
        "#  feature_columns.append(feature_column.numeric_column(header))\n",
        "feature_columns[:5]  "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[NumericColumn(key='CRIM', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='ZN', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='INDUS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='CHAS', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
              " NumericColumn(key='NOX', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L134EH-eqGQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building the model\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "model = tf.keras.Sequential([\n",
        "   feature_layer,\n",
        "   tf.keras.layers.Dense(1, activation=None)                         \n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrcis=['mse'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTfhEIHtgKYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64e54297-fc17-411e-ee79-cd09adc72bda"
      },
      "source": [
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 64ms/step - loss: 136907.7827 - val_loss: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 126439.5245 - val_loss: 113610.8333\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 119202.4500 - val_loss: 107338.4167\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 112764.0309 - val_loss: 101270.8099\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 106273.2357 - val_loss: 95516.7969\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 99844.6031 - val_loss: 89993.6771\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 94024.1924 - val_loss: 84644.3516\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 88326.1654 - val_loss: 79554.5182\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 83125.0999 - val_loss: 74740.9661\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 77827.5720 - val_loss: 70196.3151\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 72847.2294 - val_loss: 65817.7083\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 68527.1874 - val_loss: 61599.2656\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 63973.5113 - val_loss: 57687.7122\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 59769.2700 - val_loss: 53960.3945\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 55975.7397 - val_loss: 50427.6823\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 52092.3291 - val_loss: 47116.0378\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48734.3639 - val_loss: 43954.1836\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 45434.2691 - val_loss: 40982.4935\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 42213.1830 - val_loss: 38211.4596\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 39274.2860 - val_loss: 35571.3503\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 36491.6066 - val_loss: 33056.6335\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33892.0421 - val_loss: 30683.5462\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 31330.0938 - val_loss: 28472.6960\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 29078.5102 - val_loss: 26382.4160\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 26891.4274 - val_loss: 24443.4818\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24924.6218 - val_loss: 22630.6387\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 23006.7426 - val_loss: 20928.6224\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21216.9099 - val_loss: 19368.1484\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19570.0246 - val_loss: 17894.3646\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 18050.5489 - val_loss: 16494.6751\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16631.5394 - val_loss: 15216.3874\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15242.4738 - val_loss: 14042.4837\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14043.0596 - val_loss: 12918.8949\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12900.4983 - val_loss: 11905.7383\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 11837.9517 - val_loss: 10954.6901\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 10850.2625 - val_loss: 10078.3337\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9933.8950 - val_loss: 9256.3848\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 9073.7743 - val_loss: 8495.3135\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8312.4046 - val_loss: 7792.9347\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7619.2273 - val_loss: 7158.1953\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6969.6595 - val_loss: 6580.5327\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6370.5947 - val_loss: 6046.1779\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5821.5508 - val_loss: 5546.3236\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5319.6315 - val_loss: 5093.3215\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4843.1500 - val_loss: 4677.8719\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4430.4342 - val_loss: 4290.9924\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4027.2537 - val_loss: 3939.6953\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3700.9061 - val_loss: 3622.7812\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3383.8580 - val_loss: 3341.5293\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3096.9085 - val_loss: 3082.3269\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2849.8365 - val_loss: 2845.9440\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2623.0120 - val_loss: 2636.0632\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2408.7936 - val_loss: 2449.3652\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2217.6990 - val_loss: 2277.4014\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2061.7800 - val_loss: 2123.3603\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1907.1449 - val_loss: 1988.2465\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1768.4241 - val_loss: 1863.2906\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1652.3404 - val_loss: 1746.2610\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1541.4921 - val_loss: 1643.5389\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1442.6991 - val_loss: 1554.3269\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1353.9590 - val_loss: 1471.9062\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1279.5169 - val_loss: 1395.7955\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1209.7447 - val_loss: 1330.2608\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1144.3848 - val_loss: 1269.9123\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1098.4359 - val_loss: 1218.1711\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1048.0293 - val_loss: 1170.7862\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1006.9499 - val_loss: 1126.5128\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 963.7283 - val_loss: 1087.2036\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 934.0325 - val_loss: 1049.7946\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 893.3171 - val_loss: 1018.7965\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 860.1563 - val_loss: 988.5674\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 841.9433 - val_loss: 963.5667\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 818.6480 - val_loss: 938.4556\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 798.0363 - val_loss: 916.5647\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 774.9904 - val_loss: 897.0892\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 764.8763 - val_loss: 880.4477\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 755.0182 - val_loss: 863.9630\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 736.6829 - val_loss: 850.7216\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 729.6565 - val_loss: 837.8107\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 716.8198 - val_loss: 826.1433\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 703.2264 - val_loss: 814.7558\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 700.8533 - val_loss: 805.6375\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 693.1756 - val_loss: 796.8032\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 685.0117 - val_loss: 788.0930\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 677.2932 - val_loss: 779.4321\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 670.8622 - val_loss: 772.0835\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 665.4660 - val_loss: 764.5244\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 658.2697 - val_loss: 757.1523\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 653.5170 - val_loss: 750.6862\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 644.8604 - val_loss: 743.8442\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 640.9856 - val_loss: 737.5628\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 636.1335 - val_loss: 731.1247\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 626.1503 - val_loss: 725.1845\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 622.0005 - val_loss: 718.4430\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 620.0833 - val_loss: 712.0080\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 614.8581 - val_loss: 706.0075\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 610.7250 - val_loss: 700.4609\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 604.8115 - val_loss: 695.1410\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 597.5078 - val_loss: 689.6978\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 597.7108 - val_loss: 684.4337\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 591.2605 - val_loss: 679.8808\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 588.7319 - val_loss: 674.9026\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 584.2160 - val_loss: 670.2605\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 578.3582 - val_loss: 665.5406\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 574.3432 - val_loss: 660.5366\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 572.0964 - val_loss: 656.0489\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 567.9498 - val_loss: 651.7462\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.5764 - val_loss: 647.4580\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 558.4141 - val_loss: 643.3686\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 553.1628 - val_loss: 639.0576\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 551.3370 - val_loss: 634.2976\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 538.1292 - val_loss: 629.9061\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 532.6993 - val_loss: 625.2221\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 536.0948 - val_loss: 620.7440\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 534.1031 - val_loss: 616.5811\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 525.6995 - val_loss: 612.7794\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 524.6708 - val_loss: 608.4257\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 519.1719 - val_loss: 604.3372\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 501.7654 - val_loss: 600.4720\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 499.2130 - val_loss: 596.0112\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 491.2333 - val_loss: 591.5686\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 506.9173 - val_loss: 586.7027\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 482.8170 - val_loss: 582.7769\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 495.4700 - val_loss: 578.6263\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 491.6525 - val_loss: 574.5113\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 487.7923 - val_loss: 570.8320\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 486.1568 - val_loss: 566.5529\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 478.3560 - val_loss: 562.3913\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 469.3665 - val_loss: 559.1158\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 476.9020 - val_loss: 555.4734\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 469.7368 - val_loss: 551.6044\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 455.0303 - val_loss: 547.5652\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 458.3014 - val_loss: 543.5852\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 461.3832 - val_loss: 539.4626\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 458.7151 - val_loss: 535.6051\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 452.3991 - val_loss: 531.9711\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 452.7073 - val_loss: 528.3830\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 440.8762 - val_loss: 524.8423\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 445.5195 - val_loss: 520.8859\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 436.1678 - val_loss: 517.6414\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 437.3634 - val_loss: 514.1246\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 436.5585 - val_loss: 510.9025\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 432.1167 - val_loss: 507.4482\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 431.2132 - val_loss: 504.2893\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 428.8678 - val_loss: 501.2879\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 421.2393 - val_loss: 498.2745\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 423.0457 - val_loss: 496.0671\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 421.4316 - val_loss: 493.6045\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 414.1508 - val_loss: 490.6313\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 414.8709 - val_loss: 487.3226\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 401.0561 - val_loss: 484.2496\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 411.5825 - val_loss: 480.6791\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 402.9243 - val_loss: 477.7876\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 396.9681 - val_loss: 474.9440\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 403.3302 - val_loss: 471.6320\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 400.7121 - val_loss: 468.9476\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 396.8516 - val_loss: 466.3864\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 392.0756 - val_loss: 463.9747\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 392.5800 - val_loss: 461.4330\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 390.6023 - val_loss: 458.5794\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 388.0505 - val_loss: 455.9668\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 382.4291 - val_loss: 453.0917\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 384.0980 - val_loss: 450.7690\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 382.0024 - val_loss: 448.2065\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 380.2629 - val_loss: 445.7358\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 378.7809 - val_loss: 443.2204\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 375.0976 - val_loss: 440.7960\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 372.8024 - val_loss: 438.5221\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 369.4219 - val_loss: 436.3679\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 366.6101 - val_loss: 434.5502\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 367.4781 - val_loss: 432.3291\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.1560 - val_loss: 430.2390\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 359.2447 - val_loss: 428.2635\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 360.2446 - val_loss: 426.1943\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 361.2013 - val_loss: 423.7897\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 358.4701 - val_loss: 421.5444\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 356.1303 - val_loss: 419.2906\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 350.1011 - val_loss: 416.7872\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 350.5562 - val_loss: 414.7213\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 351.6859 - val_loss: 412.3735\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 348.5191 - val_loss: 410.1542\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 347.8537 - val_loss: 407.9510\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 346.5401 - val_loss: 405.8850\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 343.8044 - val_loss: 403.8990\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 341.0968 - val_loss: 401.7509\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 332.5080 - val_loss: 399.6648\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 337.7075 - val_loss: 397.5307\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 335.8496 - val_loss: 395.6009\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 334.5291 - val_loss: 393.4105\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 332.3841 - val_loss: 391.9421\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 329.8568 - val_loss: 390.2048\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 329.5208 - val_loss: 388.1956\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 329.0130 - val_loss: 386.3095\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 323.9255 - val_loss: 384.4425\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 323.3873 - val_loss: 382.3050\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 323.7638 - val_loss: 380.5321\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 310.7257 - val_loss: 378.9538\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 320.8859 - val_loss: 376.9583\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 318.2070 - val_loss: 375.2083\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 316.1593 - val_loss: 373.6456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff498e83e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3-qsQ33sFE9",
        "colab_type": "text"
      },
      "source": [
        "8) Change the model architecture by a hidden layer of 4 units and use sigmoid activation. Train the model for 200 epochs. What is the\n",
        "\t   range of the training error at the end of the training process? Try to think of why this is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8HeOLcCnxXk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1871b1fc-8ad2-4ade-e9cc-c9debab43944"
      },
      "source": [
        "#loss, mse = model.evaluate(test_ds)\n",
        "#print(\"Mean Squared Error - Test Data\", mse)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "   feature_layer,\n",
        "   tf.keras.layers.Dense(4, activation='sigmoid')                         \n",
        "])\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam', loss='mse', metrcis=['mse'])\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 62ms/step - loss: 597.7718 - val_loss: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 580.8497 - val_loss: 546.9360\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 576.3347 - val_loss: 546.9147\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 579.0013 - val_loss: 546.8695\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 578.4749 - val_loss: 546.3605\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 579.8582 - val_loss: 544.8292\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 576.2838 - val_loss: 542.1903\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 574.1434 - val_loss: 539.5838\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 569.1899 - val_loss: 539.4045\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.5828 - val_loss: 538.0836\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 569.3639 - val_loss: 535.5078\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 568.1882 - val_loss: 533.6511\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.6244 - val_loss: 532.9750\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.7883 - val_loss: 532.6339\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.8754 - val_loss: 532.4667\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.5870 - val_loss: 532.4386\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.5681 - val_loss: 532.4312\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.1495 - val_loss: 532.4286\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.5983 - val_loss: 532.4274\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.0704 - val_loss: 532.4265\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.0441 - val_loss: 532.4259\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 566.8331 - val_loss: 532.4252\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.4519 - val_loss: 532.4246\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.5256 - val_loss: 532.4239\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 559.5877 - val_loss: 532.4231\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.3404 - val_loss: 532.4224\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.9843 - val_loss: 532.4218\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.3063 - val_loss: 532.4212\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.6021 - val_loss: 532.4204\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.9731 - val_loss: 532.4197\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.8726 - val_loss: 532.4189\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 566.4523 - val_loss: 532.4181\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 566.9713 - val_loss: 532.4175\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 566.8318 - val_loss: 532.4167\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 558.9393 - val_loss: 532.4160\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.2222 - val_loss: 532.4152\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.7119 - val_loss: 532.4144\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.7915 - val_loss: 532.4137\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.2267 - val_loss: 532.4129\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.9740 - val_loss: 532.4121\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.9855 - val_loss: 532.4113\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.5011 - val_loss: 532.4103\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 559.9457 - val_loss: 532.4019\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.7997 - val_loss: 531.9491\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.8044 - val_loss: 531.7791\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.5290 - val_loss: 531.3856\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.7491 - val_loss: 530.7706\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 560.4309 - val_loss: 530.3977\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.2517 - val_loss: 530.3128\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.9512 - val_loss: 530.2480\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.2185 - val_loss: 530.1890\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.1022 - val_loss: 530.1841\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.9303 - val_loss: 530.1831\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.9020 - val_loss: 530.1828\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.1249 - val_loss: 530.1813\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.4571 - val_loss: 530.1791\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.2576 - val_loss: 530.1774\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.8223 - val_loss: 530.1765\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 560.6294 - val_loss: 530.1756\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.4858 - val_loss: 530.1747\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.5244 - val_loss: 530.1740\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.2407 - val_loss: 530.1734\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.0205 - val_loss: 530.1727\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.7347 - val_loss: 530.1723\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.2452 - val_loss: 530.1716\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.1555 - val_loss: 530.1710\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.9391 - val_loss: 530.1703\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.0892 - val_loss: 530.1698\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.1863 - val_loss: 530.1691\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.7867 - val_loss: 530.1685\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.7201 - val_loss: 530.1679\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.2923 - val_loss: 530.1675\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.0144 - val_loss: 530.1670\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 559.6624 - val_loss: 530.1664\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.0623 - val_loss: 530.1659\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.0338 - val_loss: 530.1652\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 558.2716 - val_loss: 530.1648\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.1451 - val_loss: 530.1643\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.5058 - val_loss: 530.1636\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.7021 - val_loss: 530.1630\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 556.4660 - val_loss: 530.1624\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.7088 - val_loss: 530.1619\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.2375 - val_loss: 530.1614\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 565.5271 - val_loss: 530.1608\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.1659 - val_loss: 530.1604\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.8381 - val_loss: 530.1600\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.3464 - val_loss: 530.1595\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.4059 - val_loss: 530.1590\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.3497 - val_loss: 530.1585\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.5357 - val_loss: 530.1580\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.4110 - val_loss: 530.1576\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.1366 - val_loss: 530.1570\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 555.9018 - val_loss: 530.1566\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.4847 - val_loss: 530.1560\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.8292 - val_loss: 530.1555\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.5321 - val_loss: 530.1551\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.8882 - val_loss: 530.1545\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.7428 - val_loss: 530.1540\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.2797 - val_loss: 530.1536\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.1268 - val_loss: 530.1532\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 559.8363 - val_loss: 530.1526\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.7705 - val_loss: 530.1521\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.7654 - val_loss: 530.1517\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 558.8561 - val_loss: 530.1514\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.8113 - val_loss: 530.1509\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.6833 - val_loss: 530.1505\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 559.4959 - val_loss: 530.1500\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 558.9520 - val_loss: 530.1497\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 558.4974 - val_loss: 530.1492\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 560.6608 - val_loss: 530.1488\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.2289 - val_loss: 530.1484\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.4406 - val_loss: 530.1481\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 556.2847 - val_loss: 530.1476\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.2367 - val_loss: 530.1472\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.9040 - val_loss: 530.1469\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 559.8555 - val_loss: 530.1464\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.8024 - val_loss: 530.1459\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.3377 - val_loss: 530.1455\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.6977 - val_loss: 530.1452\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.0502 - val_loss: 530.1449\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.3190 - val_loss: 530.1444\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.1295 - val_loss: 530.1441\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.6545 - val_loss: 530.1437\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.6297 - val_loss: 530.1433\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 559.0875 - val_loss: 530.1430\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.8747 - val_loss: 530.1427\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.8195 - val_loss: 530.1423\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 557.8079 - val_loss: 530.1419\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.2114 - val_loss: 530.1415\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.7620 - val_loss: 530.1412\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.2113 - val_loss: 530.1409\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.2788 - val_loss: 530.1405\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.6173 - val_loss: 530.1401\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.4316 - val_loss: 530.1398\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.2274 - val_loss: 530.1395\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.7505 - val_loss: 530.1392\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 558.3205 - val_loss: 530.1388\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 557.7190 - val_loss: 530.1385\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.9662 - val_loss: 530.1380\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.5597 - val_loss: 530.1358\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.2722 - val_loss: 530.1347\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.0322 - val_loss: 530.1325\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.8487 - val_loss: 530.1318\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.9140 - val_loss: 530.1313\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.0275 - val_loss: 530.1311\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.0082 - val_loss: 530.1308\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.3040 - val_loss: 530.1305\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.8526 - val_loss: 530.1304\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.7085 - val_loss: 530.1301\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.9032 - val_loss: 530.1299\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.9032 - val_loss: 530.1296\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 560.6202 - val_loss: 530.1295\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.2896 - val_loss: 530.1292\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.8836 - val_loss: 530.1290\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.7062 - val_loss: 530.1287\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 560.5020 - val_loss: 530.1286\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 565.2935 - val_loss: 530.1283\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.1052 - val_loss: 530.1281\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 558.7338 - val_loss: 530.1279\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.6600 - val_loss: 530.1277\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.0402 - val_loss: 530.1274\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.0324 - val_loss: 530.1272\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 558.3866 - val_loss: 530.1270\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.9584 - val_loss: 530.1268\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 559.5884 - val_loss: 530.1267\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.2242 - val_loss: 530.1265\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 559.6787 - val_loss: 530.1262\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 565.5396 - val_loss: 530.1260\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 558.5677 - val_loss: 530.1258\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.5428 - val_loss: 530.1257\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.2011 - val_loss: 530.1255\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.8444 - val_loss: 530.1253\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 561.7932 - val_loss: 530.1250\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.5127 - val_loss: 530.1248\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.0092 - val_loss: 530.1246\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 565.3121 - val_loss: 530.1244\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.4658 - val_loss: 530.1243\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.6563 - val_loss: 530.1241\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.5237 - val_loss: 530.1238\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.2406 - val_loss: 530.1237\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.6137 - val_loss: 530.1234\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.7177 - val_loss: 530.1233\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.4208 - val_loss: 530.1230\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 552.8571 - val_loss: 530.1229\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.8857 - val_loss: 530.1227\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.4193 - val_loss: 530.1226\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.1998 - val_loss: 530.1224\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.4949 - val_loss: 530.1222\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 564.4472 - val_loss: 530.1220\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 562.4598 - val_loss: 530.1218\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 560.6870 - val_loss: 530.1216\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.7307 - val_loss: 530.1214\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 563.6864 - val_loss: 530.1213\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.7236 - val_loss: 530.1211\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 564.7973 - val_loss: 530.1209\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 563.5842 - val_loss: 530.1207\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 561.8726 - val_loss: 530.1206\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.8485 - val_loss: 530.1205\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 560.1053 - val_loss: 530.1203\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 562.3108 - val_loss: 530.1201\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4a4578710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSV4RJIDs2sg",
        "colab_type": "text"
      },
      "source": [
        "9) We will now experiment by changing the loss function. First, remove the hidden layer that we added for Q8. Then, try the solutions\n",
        "\t   of the Q6 as the loss function for the optimizer. What is the mean squared error on the test dataset for the best model? (approx)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQB7JTrss3a_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3933150-361a-414c-8185-fdd53f5abe05"
      },
      "source": [
        "model_3 = tf.keras.Sequential([\n",
        "   feature_layer,\n",
        "   tf.keras.layers.Dense(1, activation=None)                         \n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_3.compile(optimizer='adam', loss='mse', metrcis=['mse', 'mae'])\n",
        "model_3.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 81ms/step - loss: 29777.7112 - val_loss: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 27764.9200 - val_loss: 22820.7819\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 24522.5009 - val_loss: 20157.6302\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21722.8178 - val_loss: 17691.7539\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 19043.9377 - val_loss: 15487.7900\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 16673.3612 - val_loss: 13493.2744\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 14473.3817 - val_loss: 11702.0329\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 12556.9229 - val_loss: 10091.3613\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 10885.0579 - val_loss: 8673.7979\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9330.0169 - val_loss: 7462.7744\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 8086.2713 - val_loss: 6386.5275\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 6909.2976 - val_loss: 5481.3504\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 5946.3553 - val_loss: 4677.3523\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5080.5181 - val_loss: 4000.9652\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 4349.7593 - val_loss: 3429.0537\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3726.4591 - val_loss: 2936.4360\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3175.4481 - val_loss: 2520.9001\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2715.9974 - val_loss: 2162.6731\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2305.4888 - val_loss: 1856.7580\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2003.9247 - val_loss: 1598.7489\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1696.9731 - val_loss: 1402.5486\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1506.5604 - val_loss: 1232.0598\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1306.9995 - val_loss: 1105.4555\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1168.4017 - val_loss: 1000.3573\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1056.9748 - val_loss: 920.4516\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 966.3609 - val_loss: 857.5616\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 890.6512 - val_loss: 807.3659\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 818.6320 - val_loss: 764.3590\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 767.6743 - val_loss: 725.4982\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 716.9673 - val_loss: 695.5269\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 683.9299 - val_loss: 671.7781\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 659.7893 - val_loss: 653.4067\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 637.6254 - val_loss: 640.3075\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 616.6638 - val_loss: 629.2574\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 605.2438 - val_loss: 619.9074\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 588.8747 - val_loss: 610.3478\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 580.4973 - val_loss: 601.9705\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 568.8020 - val_loss: 594.6145\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 555.7571 - val_loss: 587.2639\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 552.1506 - val_loss: 580.5163\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 533.1517 - val_loss: 574.3348\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 524.2298 - val_loss: 567.4352\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 526.1680 - val_loss: 559.9403\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 513.9588 - val_loss: 553.6554\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 512.4784 - val_loss: 548.0001\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 502.1543 - val_loss: 541.7897\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 497.8913 - val_loss: 534.6093\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 492.9322 - val_loss: 527.6676\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 488.6057 - val_loss: 521.0203\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 478.7701 - val_loss: 514.8615\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 477.5879 - val_loss: 508.8228\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 461.1386 - val_loss: 502.4070\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 458.3513 - val_loss: 495.7193\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 454.7685 - val_loss: 488.5827\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 447.9253 - val_loss: 481.7784\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 442.6681 - val_loss: 474.9981\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 439.2640 - val_loss: 468.6236\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 434.3334 - val_loss: 463.4455\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 431.0181 - val_loss: 457.8202\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 423.5411 - val_loss: 452.4407\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 421.5625 - val_loss: 447.8651\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 416.3460 - val_loss: 443.1649\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 411.3981 - val_loss: 438.0632\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 406.0245 - val_loss: 432.8948\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 402.3695 - val_loss: 427.5619\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 396.8756 - val_loss: 422.4184\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 386.4681 - val_loss: 417.5627\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 382.5789 - val_loss: 412.0164\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 382.7668 - val_loss: 406.2704\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 379.2076 - val_loss: 400.9961\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 370.5209 - val_loss: 396.4462\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.1598 - val_loss: 391.5367\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 365.1652 - val_loss: 386.1992\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 361.7822 - val_loss: 381.6906\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 357.4962 - val_loss: 378.1686\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 354.1445 - val_loss: 373.9463\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 348.6171 - val_loss: 369.7257\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 344.0796 - val_loss: 365.6102\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 340.6556 - val_loss: 361.9195\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 332.2360 - val_loss: 357.9231\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 334.2653 - val_loss: 352.8399\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 331.0661 - val_loss: 348.9598\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 328.5935 - val_loss: 344.9081\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 321.9675 - val_loss: 341.3044\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 319.7954 - val_loss: 337.7257\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 313.6388 - val_loss: 334.0023\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 313.9833 - val_loss: 331.1472\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 305.6829 - val_loss: 327.5505\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 305.1313 - val_loss: 322.8090\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 305.6197 - val_loss: 319.1877\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 299.4540 - val_loss: 316.2222\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 299.7006 - val_loss: 312.9667\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 295.6243 - val_loss: 309.8348\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 291.2095 - val_loss: 306.7226\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 290.7008 - val_loss: 303.1383\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 285.3949 - val_loss: 299.7022\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 285.1609 - val_loss: 297.4165\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 283.0126 - val_loss: 294.9558\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 280.7426 - val_loss: 292.1505\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 277.4836 - val_loss: 289.5969\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 274.8284 - val_loss: 286.6718\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 273.0110 - val_loss: 283.5003\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 270.7404 - val_loss: 280.7098\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 268.1667 - val_loss: 278.6947\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 266.2085 - val_loss: 276.8631\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 261.4625 - val_loss: 273.8049\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 262.2026 - val_loss: 270.5937\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 259.4384 - val_loss: 267.9745\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 253.6929 - val_loss: 265.9040\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 254.0884 - val_loss: 263.3114\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 254.3773 - val_loss: 261.5996\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 250.5487 - val_loss: 259.0970\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 249.6166 - val_loss: 256.6266\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 245.7880 - val_loss: 254.0575\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 243.7237 - val_loss: 251.1614\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 243.4053 - val_loss: 248.5352\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 242.1817 - val_loss: 246.3954\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 240.3380 - val_loss: 244.5937\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 235.5529 - val_loss: 242.6107\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 235.3426 - val_loss: 240.9400\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 234.8004 - val_loss: 239.6962\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 229.3613 - val_loss: 237.9821\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 228.2599 - val_loss: 236.1615\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 228.6238 - val_loss: 233.8942\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 226.4940 - val_loss: 232.2155\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 225.5618 - val_loss: 229.6961\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 222.9379 - val_loss: 227.6103\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 220.8477 - val_loss: 225.6851\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 220.8299 - val_loss: 223.3472\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 214.9805 - val_loss: 221.5842\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 218.5235 - val_loss: 220.0223\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 216.2303 - val_loss: 218.4590\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 216.7546 - val_loss: 216.6782\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 213.9823 - val_loss: 215.1147\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 212.1901 - val_loss: 213.6366\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 211.0635 - val_loss: 212.2718\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 206.8242 - val_loss: 210.9512\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 207.7608 - val_loss: 209.7320\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 204.3319 - val_loss: 208.3299\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 205.3800 - val_loss: 206.9922\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 204.2555 - val_loss: 205.5081\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 202.0908 - val_loss: 204.2656\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 200.0604 - val_loss: 203.2732\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 200.1937 - val_loss: 202.0977\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 197.7846 - val_loss: 200.2987\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 198.1574 - val_loss: 199.5192\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 193.8868 - val_loss: 197.1509\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 195.5433 - val_loss: 194.6480\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 192.8229 - val_loss: 193.0776\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 188.6200 - val_loss: 191.7121\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 189.3189 - val_loss: 190.1972\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 188.4489 - val_loss: 188.6799\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 188.8789 - val_loss: 187.2466\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 187.9842 - val_loss: 186.0267\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 186.1170 - val_loss: 184.8640\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 179.0256 - val_loss: 183.5927\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 182.2346 - val_loss: 182.3584\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 183.1919 - val_loss: 180.9823\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 182.1426 - val_loss: 179.8401\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 180.4516 - val_loss: 179.0906\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 178.2950 - val_loss: 178.2330\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 178.6276 - val_loss: 176.7150\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 177.4286 - val_loss: 175.4763\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 175.4875 - val_loss: 174.1321\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 175.5364 - val_loss: 172.9811\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 173.1605 - val_loss: 171.8462\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 173.5832 - val_loss: 170.5787\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 170.5915 - val_loss: 169.4393\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 170.0283 - val_loss: 168.5125\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 169.3534 - val_loss: 167.9019\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 166.6458 - val_loss: 167.5367\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 169.0021 - val_loss: 167.0537\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 165.0944 - val_loss: 165.5366\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.8752 - val_loss: 163.5989\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 164.4310 - val_loss: 161.6517\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 162.7103 - val_loss: 160.3667\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 163.1310 - val_loss: 159.3055\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 161.9457 - val_loss: 158.2774\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 160.3050 - val_loss: 157.5864\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 158.5425 - val_loss: 156.8273\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 158.0245 - val_loss: 156.0713\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 157.9574 - val_loss: 155.4390\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 157.0649 - val_loss: 154.1742\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 155.7753 - val_loss: 152.9412\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 153.9770 - val_loss: 151.9510\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 154.7663 - val_loss: 151.3587\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 154.1154 - val_loss: 149.8978\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 152.1973 - val_loss: 148.8769\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 151.8765 - val_loss: 148.0604\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.9829 - val_loss: 147.1995\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 150.3464 - val_loss: 146.4102\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 148.9082 - val_loss: 145.3151\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 148.6950 - val_loss: 144.0216\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 147.5446 - val_loss: 143.2019\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 146.6158 - val_loss: 142.1487\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 145.3036 - val_loss: 141.2823\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 143.3762 - val_loss: 140.7909\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 143.5930 - val_loss: 139.5776\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 143.0411 - val_loss: 138.8337\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 141.2109 - val_loss: 138.3689\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff49d2094e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTUXK6JyuFrf",
        "colab_type": "text"
      },
      "source": [
        "10) We will now try to bucketize one of the feature columns and see its effect on the model’s performance. Bucketize the ‘RAD’ column\n",
        "\t   with the ​ boundaries ​ parameter as [2, 5] and retrain the model for 200 epochs. The performance on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQBrFFWnn-DO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffef6ea1-697e-414c-a151-0eba413da891"
      },
      "source": [
        "rad = feature_column.numeric_column('RAD')\n",
        "rad_buckets = feature_column.bucketized_column(rad, boundaries=[2, 5])\n",
        "feature_columns.append(rad_buckets)\n",
        "\n",
        "# Building the model\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
        "model_4 = tf.keras.Sequential([\n",
        "   feature_layer,\n",
        "   tf.keras.layers.Dense(1, activation=None)                         \n",
        "])\n",
        "\n",
        "# compile model\n",
        "model_4.compile(optimizer='adam', loss='mse', metrcis=['mse'])\n",
        "model_4.fit(train_ds, validation_data=val_ds, epochs=200)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n",
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 88ms/step - loss: 17003.0692 - val_loss: 0.0000e+00\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15094.3410 - val_loss: 14485.7578\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 13136.0439 - val_loss: 12664.8984\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 11283.4205 - val_loss: 11098.3031\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 9812.8095 - val_loss: 9730.8708\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 8518.1275 - val_loss: 8582.3078\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 7425.3018 - val_loss: 7628.9272\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 6570.4477 - val_loss: 6835.5968\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5774.3952 - val_loss: 6185.7749\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 5223.6444 - val_loss: 5628.6561\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4737.7768 - val_loss: 5189.8418\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4333.5221 - val_loss: 4828.6159\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 4040.8611 - val_loss: 4554.2514\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3799.6687 - val_loss: 4303.6372\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3635.1081 - val_loss: 4106.0347\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 3409.7519 - val_loss: 3934.4801\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3303.7421 - val_loss: 3784.9298\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3193.4850 - val_loss: 3648.6688\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 3057.4753 - val_loss: 3532.5029\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2985.8554 - val_loss: 3423.5259\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2894.4471 - val_loss: 3315.8724\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2783.0922 - val_loss: 3217.3750\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2711.7049 - val_loss: 3122.6238\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2642.7101 - val_loss: 3030.8302\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2566.2144 - val_loss: 2944.4448\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2495.2332 - val_loss: 2861.8477\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 2413.2692 - val_loss: 2786.0050\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2356.8372 - val_loss: 2711.7474\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2291.0448 - val_loss: 2639.7986\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2231.9136 - val_loss: 2567.2997\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2149.1746 - val_loss: 2498.3425\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2094.9439 - val_loss: 2426.9305\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 2029.0178 - val_loss: 2356.1871\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1975.1400 - val_loss: 2288.0092\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1910.7124 - val_loss: 2222.8323\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1860.5753 - val_loss: 2158.5883\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1789.6577 - val_loss: 2095.7679\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1737.1510 - val_loss: 2029.7800\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1685.4361 - val_loss: 1967.3293\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1614.4511 - val_loss: 1907.6347\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1553.5427 - val_loss: 1844.3530\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1520.5057 - val_loss: 1784.9666\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1471.1393 - val_loss: 1728.4055\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1416.2584 - val_loss: 1671.7838\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1374.9379 - val_loss: 1618.2725\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1330.0745 - val_loss: 1570.4083\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1281.8757 - val_loss: 1521.6730\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1238.5832 - val_loss: 1472.9029\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1201.1963 - val_loss: 1422.6520\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1140.1972 - val_loss: 1375.6915\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1110.4322 - val_loss: 1329.1145\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1073.0564 - val_loss: 1283.8542\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1031.7597 - val_loss: 1239.3269\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 1006.3725 - val_loss: 1196.4188\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 963.3009 - val_loss: 1155.6972\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 936.3347 - val_loss: 1115.3593\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 904.6779 - val_loss: 1077.3030\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 862.0554 - val_loss: 1040.6132\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 840.1674 - val_loss: 1003.3722\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 801.9212 - val_loss: 968.5046\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 786.6518 - val_loss: 933.5208\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 754.0012 - val_loss: 903.4453\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 728.4386 - val_loss: 874.0193\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 706.6667 - val_loss: 844.0882\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 678.3283 - val_loss: 814.9842\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 662.9853 - val_loss: 785.0717\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 634.3027 - val_loss: 758.5019\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 616.0793 - val_loss: 732.1410\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 596.6513 - val_loss: 706.9684\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 577.2037 - val_loss: 683.3404\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 550.0668 - val_loss: 661.0596\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 535.0830 - val_loss: 636.9176\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 522.9622 - val_loss: 615.6708\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 505.9484 - val_loss: 594.6638\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 488.0182 - val_loss: 574.4622\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 468.1371 - val_loss: 555.2981\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 457.4282 - val_loss: 535.2921\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 447.9933 - val_loss: 517.2373\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 425.5997 - val_loss: 501.1764\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 420.1049 - val_loss: 483.3760\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 406.3524 - val_loss: 468.1337\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 394.7404 - val_loss: 453.3398\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 384.7205 - val_loss: 439.0222\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 366.8887 - val_loss: 424.5770\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 364.0815 - val_loss: 409.2600\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 351.8979 - val_loss: 395.8004\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 343.0012 - val_loss: 383.3772\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 333.1617 - val_loss: 371.5716\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 323.3337 - val_loss: 361.2005\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 319.7440 - val_loss: 350.8511\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 310.8503 - val_loss: 341.2449\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 305.2992 - val_loss: 331.5002\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 298.9414 - val_loss: 322.1426\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 290.5387 - val_loss: 313.4957\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 286.1119 - val_loss: 305.6930\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 281.3449 - val_loss: 298.2134\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 272.8418 - val_loss: 290.1745\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 268.5060 - val_loss: 281.6072\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 263.7230 - val_loss: 274.6094\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 255.4457 - val_loss: 267.7977\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 253.1208 - val_loss: 261.0352\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 249.2145 - val_loss: 253.8188\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 244.3261 - val_loss: 246.9569\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 238.3575 - val_loss: 240.7701\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 234.0724 - val_loss: 234.8272\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 231.8195 - val_loss: 229.1494\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 228.2303 - val_loss: 223.5088\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 224.8235 - val_loss: 218.4151\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 220.0434 - val_loss: 213.6976\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 217.5463 - val_loss: 208.5914\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 214.2596 - val_loss: 203.8514\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 211.0629 - val_loss: 199.2802\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 208.0234 - val_loss: 195.0440\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 199.9201 - val_loss: 190.8871\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 199.8904 - val_loss: 186.9441\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 198.8637 - val_loss: 183.5306\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 194.9215 - val_loss: 179.7623\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 193.1298 - val_loss: 175.8502\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 189.3318 - val_loss: 172.4891\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 187.7853 - val_loss: 168.7503\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 184.0220 - val_loss: 165.5607\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 181.0611 - val_loss: 162.5606\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 179.2879 - val_loss: 159.4392\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 179.1601 - val_loss: 156.4511\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 176.9633 - val_loss: 153.6152\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 175.0402 - val_loss: 151.0153\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 172.7497 - val_loss: 148.5130\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 171.1720 - val_loss: 146.0393\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 158.7253 - val_loss: 143.7082\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 168.0631 - val_loss: 142.4014\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 165.7637 - val_loss: 140.0767\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 163.8950 - val_loss: 137.9992\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 160.7102 - val_loss: 136.0005\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 160.8219 - val_loss: 134.1933\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 158.8307 - val_loss: 131.7405\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 156.2882 - val_loss: 129.4287\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 156.3428 - val_loss: 126.6797\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 154.2516 - val_loss: 124.5838\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 152.2214 - val_loss: 122.7826\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 148.9365 - val_loss: 120.8422\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.2130 - val_loss: 120.0724\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 148.4552 - val_loss: 119.2387\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 146.3791 - val_loss: 117.3978\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 146.5405 - val_loss: 115.7819\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 145.5846 - val_loss: 114.1959\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 143.8117 - val_loss: 112.9791\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 142.9071 - val_loss: 111.6000\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 141.9504 - val_loss: 110.2319\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 140.9626 - val_loss: 108.8026\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 139.8586 - val_loss: 107.6302\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 138.5979 - val_loss: 106.4642\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 137.5282 - val_loss: 105.3542\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 136.8197 - val_loss: 104.3902\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 132.1117 - val_loss: 103.3278\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 135.0899 - val_loss: 102.5862\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 133.4831 - val_loss: 101.6180\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 132.9285 - val_loss: 100.4457\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 131.9839 - val_loss: 99.3987\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 130.7474 - val_loss: 98.6207\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 129.7278 - val_loss: 98.0153\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 129.3752 - val_loss: 97.0798\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 128.3379 - val_loss: 96.1942\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 127.6627 - val_loss: 95.5901\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 126.9299 - val_loss: 94.6016\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 125.5107 - val_loss: 93.3129\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 124.1816 - val_loss: 92.4062\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 124.2662 - val_loss: 91.4736\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 122.9418 - val_loss: 90.6499\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 122.7803 - val_loss: 89.8864\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 122.0935 - val_loss: 88.9612\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 121.7290 - val_loss: 88.2681\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 120.1604 - val_loss: 87.5906\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 118.7578 - val_loss: 87.0530\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 118.2176 - val_loss: 86.3744\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 117.1890 - val_loss: 85.5454\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 112.9730 - val_loss: 85.0197\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 116.2828 - val_loss: 85.4274\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 114.9134 - val_loss: 85.1314\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 114.5526 - val_loss: 84.0876\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 114.4833 - val_loss: 83.4838\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 112.7131 - val_loss: 82.8435\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 112.8666 - val_loss: 82.3328\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 111.8259 - val_loss: 81.6427\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 111.6444 - val_loss: 80.9694\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 110.9579 - val_loss: 80.3315\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 110.0273 - val_loss: 79.7875\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 109.2601 - val_loss: 78.9267\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 109.4930 - val_loss: 78.3488\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 108.5308 - val_loss: 77.7561\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 108.4242 - val_loss: 77.3015\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 103.2257 - val_loss: 76.9389\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 106.5946 - val_loss: 78.2154\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 106.4017 - val_loss: 77.2038\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 105.5463 - val_loss: 76.2150\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 105.6385 - val_loss: 75.7364\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 105.3170 - val_loss: 75.0655\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 104.4129 - val_loss: 74.5865\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 101.2353 - val_loss: 74.3656\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 100.7622 - val_loss: 75.1757\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 102.8164 - val_loss: 76.2879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4985b7320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R53J7RMLvEXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}